{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import nn, optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "X = torch.tensor(np.arange(0, 31), dtype=torch.float32).view(-1, 1)\n",
    "Y = torch.tensor(np.array([30, 35, 33, 32, 34, 37, 39, 38, 36, 36, 37, 39, 42, 45, 45, 41, 40, 39, 42, 44, 47, 49, 50, 49, 46, 48, 50, 53, 55, 54, 53]), dtype=torch.float32).view(-1, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(1, 32)\n",
    "        self.layer2 = nn.Linear(32, 64)\n",
    "        self.layer3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def train_and_test_nn(train_indices, test_indices, epochs):\n",
    "    model = NeuralNetwork()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_X = X[train_indices]\n",
    "    train_Y = Y[train_indices]\n",
    "    test_X = X[test_indices]\n",
    "    test_Y = Y[test_indices]\n",
    "\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_X)\n",
    "        loss = criterion(output, train_Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch % 100) == 0:\n",
    "            print(f\"Epoch {epoch} loss: {loss}\")\n",
    "\n",
    "    preds_train = model(train_X)\n",
    "    train_loss = criterion(preds_train, train_Y)\n",
    "\n",
    "    preds_test = model(test_X)\n",
    "    test_loss = criterion(preds_test, test_Y)\n",
    "    return train_loss, test_loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case (ii): First 20 points as training data\n",
      "Epoch 0 loss: 1408.250244140625\n",
      "Epoch 100 loss: 5.858303070068359\n",
      "Epoch 200 loss: 5.080464839935303\n",
      "Epoch 300 loss: 4.916697978973389\n",
      "Epoch 400 loss: 4.752251625061035\n",
      "Epoch 500 loss: 4.680600643157959\n",
      "Epoch 600 loss: 4.656827926635742\n",
      "Epoch 700 loss: 4.641770362854004\n",
      "Epoch 800 loss: 4.622031211853027\n",
      "Epoch 900 loss: 4.598869323730469\n",
      "Epoch 1000 loss: 4.574982166290283\n",
      "Epoch 1100 loss: 4.553575038909912\n",
      "Epoch 1200 loss: 4.53945779800415\n",
      "Epoch 1300 loss: 4.535706043243408\n",
      "Epoch 1400 loss: 4.54232931137085\n",
      "Epoch 1500 loss: 4.538817405700684\n",
      "Epoch 1600 loss: 4.669854640960693\n",
      "Epoch 1700 loss: 4.6919403076171875\n",
      "Epoch 1800 loss: 4.530725955963135\n",
      "Epoch 1900 loss: 4.530873775482178\n",
      "\n",
      "Train Error: 4.737574577331543\n",
      "Test Error: 6.215279579162598\n",
      "\n",
      "Case (iii): First 10 and last 10 points as training data\n",
      "Epoch 0 loss: 2047.760986328125\n",
      "Epoch 100 loss: 3.8218345642089844\n",
      "Epoch 200 loss: 3.124260425567627\n",
      "Epoch 300 loss: 2.7281723022460938\n",
      "Epoch 400 loss: 2.4249110221862793\n",
      "Epoch 500 loss: 2.227436065673828\n",
      "Epoch 600 loss: 2.1628828048706055\n",
      "Epoch 700 loss: 2.1287589073181152\n",
      "Epoch 800 loss: 2.0944018363952637\n",
      "Epoch 900 loss: 2.056519031524658\n",
      "Epoch 1000 loss: 2.0614352226257324\n",
      "Epoch 1100 loss: 1.9709501266479492\n",
      "Epoch 1200 loss: 1.9450737237930298\n",
      "Epoch 1300 loss: 1.9940156936645508\n",
      "Epoch 1400 loss: 1.9012187719345093\n",
      "Epoch 1500 loss: 1.8679075241088867\n",
      "Epoch 1600 loss: 1.9033313989639282\n",
      "Epoch 1700 loss: 1.8417894840240479\n",
      "Epoch 1800 loss: 1.9970893859863281\n",
      "Epoch 1900 loss: 1.8381969928741455\n",
      "\n",
      "Train Error: 1.8540862798690796\n",
      "Test Error: 7.464297294616699\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Case (ii): First 20 points as training data\")\n",
    "train_error, test_error = train_and_test_nn(np.arange(20), np.arange(20, 30), 2000)\n",
    "print(f\"\\nTrain Error: {train_error}\\nTest Error: {test_error}\")\n",
    "\n",
    "print(\"\\nCase (iii): First 10 and last 10 points as training data\")\n",
    "train_error2, test_error2 = train_and_test_nn(np.hstack([np.arange(10), np.arange(21, 31)]), np.arange(10, 21), 2000)\n",
    "print(f\"\\nTrain Error: {train_error2}\\nTest Error: {test_error2}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "#HW 1 Scores:\n",
    "#1\n",
    "# Error for Line (Train): 2.242749386808538\n",
    "# Error for Parabola (Train): 2.1255393482773766\n",
    "# Error for Polynomial (Train): 0.028351503968806435\n",
    "# Error for Line (Test): 3.36363873604787\n",
    "# Error for Parabola (Test): 8.713651781874919\n",
    "# Error for Polynomial (Test): 28617752784.428474\n",
    "\n",
    "#2\n",
    "# Error for Line (Train): 1.851669904329375\n",
    "# Error for Parabola (Train): 1.8508364115957907\n",
    "# Error for Polynomial (Train): 0.1638133765080727\n",
    "# Error for Line (Test): 2.8065076975181618\n",
    "# Error for Parabola (Test): 2.774982896893291\n",
    "# Error for Polynomial (Test): 483.9099124568562"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for Line (Train): 2.242749386808538\n",
      "Error for Parabola (Train): 2.1255393482773766\n",
      "Error for Polynomial (Train): 0.028351503968806435\n",
      "Error for Line (Test): 3.36363873604787\n",
      "Error for Parabola (Test): 8.713651781874919\n",
      "Error for Polynomial (Test): 28617752784.428474\n",
      "Error for Line (Train): 1.851669904329375\n",
      "Error for Parabola (Train): 1.8508364115957907\n",
      "Error for Polynomial (Train): 0.1638133765080727\n",
      "Error for Line (Test): 2.8065076975181618\n",
      "Error for Parabola (Test): 2.774982896893291\n",
      "Error for Polynomial (Test): 483.9099124568562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jleus\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3369: RankWarning: Polyfit may be poorly conditioned\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\jleus\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3369: RankWarning: Polyfit may be poorly conditioned\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#HW1\n",
    "X = np.arange(0, 31)\n",
    "Y = np.array([30, 35, 33, 32, 34, 37, 39, 38, 36, 36, 37, 39, 42, 45, 45, 41,\n",
    "40, 39, 42, 44, 47, 49, 50, 49, 46, 48, 50, 53, 55, 54, 53])\n",
    "X_train = X[:20]\n",
    "X_test = X[20:]\n",
    "Y_train = Y[:20]\n",
    "Y_test = Y[20:]\n",
    "def calc_error(coeffs, X, Y):\n",
    "    function = np.poly1d(coeffs)\n",
    "    pred = function(X)\n",
    "    errors2 = (pred - Y) ** 2\n",
    "    return math.sqrt(np.sum(errors2) / len(X))\n",
    "line = np.polyfit(X_train, Y_train, 1)\n",
    "parabola = np.polyfit(X_train, Y_train, 2)\n",
    "poly = np.polyfit(X_train, Y_train, 19)\n",
    "\n",
    "error_line_train = calc_error(line, X_train, Y_train)\n",
    "error_parabola_train = calc_error(parabola, X_train, Y_train)\n",
    "error_poly_train = calc_error(poly, X_train, Y_train)\n",
    "error_line_test = calc_error(line, X_test, Y_test)\n",
    "error_parabola_test = calc_error(parabola, X_test, Y_test)\n",
    "error_poly_test = calc_error(poly, X_test, Y_test)\n",
    "error_vals = [error_line_train, error_parabola_train, error_poly_train,\n",
    "              error_line_test, error_parabola_test, error_poly_test]\n",
    "names2 = [\"Line (Train)\", \"Parabola (Train)\", \"Polynomial (Train)\",\n",
    "          \"Line (Test)\", \"Parabola (Test)\", \"Polynomial (Test)\"]\n",
    "for i in range(6):\n",
    "    print(f\"Error for {names2[i]}: {error_vals[i]}\")\n",
    "\n",
    "#Part II (iv)\n",
    "X_train2 = X[:10]\n",
    "X_train2 = np.concatenate((X_train2, X[21:]))\n",
    "Y_train2 = Y[:10]\n",
    "Y_train2 = np.concatenate((Y_train2, Y[21:]))\n",
    "X_test2 = X[10:21]\n",
    "Y_test2 = Y[10:21]\n",
    "line2 = np.polyfit(X_train2, Y_train2, 1)\n",
    "parabola2 = np.polyfit(X_train2, Y_train2, 2)\n",
    "poly2 = np.polyfit(X_train2, Y_train2, 19)\n",
    "error_line_train2 = calc_error(line2, X_train2, Y_train2)\n",
    "error_parabola_train2 = calc_error(parabola2, X_train2, Y_train2)\n",
    "error_poly_train2 = calc_error(poly2, X_train2, Y_train2)\n",
    "error_line_test2 = calc_error(line2, X_test2, Y_test2)\n",
    "error_parabola_test2 = calc_error(parabola2, X_test2, Y_test2)\n",
    "error_poly_test2 = calc_error(poly2, X_test2, Y_test2)\n",
    "error_vals = [error_line_train2, error_parabola_train2, error_poly_train2,\n",
    "              error_line_test2, error_parabola_test2, error_poly_test2]\n",
    "names3 = [\"Line (Train)\", \"Parabola (Train)\", \"Polynomial (Train)\",\n",
    "          \"Line (Test)\", \"Parabola (Test)\", \"Polynomial (Test)\"]\n",
    "for i in range(6):\n",
    "    print(f\"Error for {names2[i]}: {error_vals[i]}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 PCA Modes:\n",
      "[[ 5.58521229e-18 -4.33747086e-19  4.50231512e-19 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.08824866e-16 -1.56249716e-17  1.50361393e-17 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 2.05768397e-17  3.22957539e-17 -3.94120299e-17 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " ...\n",
      " [ 5.15809189e-17  6.45184807e-17  2.06693207e-16 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 5.28105334e-17 -5.91629520e-17 -5.12231420e-17 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-6.71233474e-17  5.03922476e-17  8.40276715e-17 ... -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]]\n",
      "Epoch [1/10], Step [100/938], Loss: 1.9406\n",
      "Epoch [1/10], Step [200/938], Loss: 1.2251\n",
      "Epoch [1/10], Step [300/938], Loss: 0.8189\n",
      "Epoch [1/10], Step [400/938], Loss: 0.6054\n",
      "Epoch [1/10], Step [500/938], Loss: 0.3623\n",
      "Epoch [1/10], Step [600/938], Loss: 0.4840\n",
      "Epoch [1/10], Step [700/938], Loss: 0.4193\n",
      "Epoch [1/10], Step [800/938], Loss: 0.3262\n",
      "Epoch [1/10], Step [900/938], Loss: 0.3280\n",
      "Epoch [2/10], Step [100/938], Loss: 0.4320\n",
      "Epoch [2/10], Step [200/938], Loss: 0.3178\n",
      "Epoch [2/10], Step [300/938], Loss: 0.5192\n",
      "Epoch [2/10], Step [400/938], Loss: 0.3999\n",
      "Epoch [2/10], Step [500/938], Loss: 0.2224\n",
      "Epoch [2/10], Step [600/938], Loss: 0.3327\n",
      "Epoch [2/10], Step [700/938], Loss: 0.1817\n",
      "Epoch [2/10], Step [800/938], Loss: 0.3386\n",
      "Epoch [2/10], Step [900/938], Loss: 0.2407\n",
      "Epoch [3/10], Step [100/938], Loss: 0.3124\n",
      "Epoch [3/10], Step [200/938], Loss: 0.3307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST train and test datasets\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = torchvision.datasets.MNIST('~/.torch/datasets', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST('~/.torch/datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Compute the first 20 PCA modes of the digit images\n",
    "train_data = train_dataset.data.numpy().reshape(-1, 28*28)\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(train_data)\n",
    "print(f'First 20 PCA Modes:\\n{pca.components_}')\n",
    "\n",
    "# Loaders for the train and test datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000)\n",
    "\n",
    "# Define the neural network\n",
    "class FeedForwardNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 128)\n",
    "        self.fc3 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = FeedForwardNN()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(train_loader):\n",
    "        # Reshape the data and perform the forward pass\n",
    "        data = data.view(data.size(0), -1)\n",
    "        predictions = model(data)\n",
    "\n",
    "        # Compute the loss, gradients, and update the parameters\n",
    "        loss = criterion(predictions, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Test the neural network\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        # Reshape the data and perform the forward pass\n",
    "        data = data.view(data.size(0), -1)\n",
    "        predictions = model(data)\n",
    "\n",
    "        # Compute the number of correct predictions\n",
    "        _, predicted = torch.max(predictions.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "#HW3 Scores:\n",
    "#Tree: 0.8769\n",
    "#SVM: 0.9792\n",
    "#LDA: 0.8730\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "#HW3\n",
    "\n",
    "#SVM LDA and Decision Trees\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 784).T\n",
    "X_test = X_test.reshape(10000, 784).T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8769\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(random_state=44)\n",
    "tree.fit(X_train.T, y_train)\n",
    "y_pred_tree = tree.predict(X_test.T)\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "print(accuracy_tree)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9792\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(probability=False)\n",
    "svm.fit(X_train.T, y_train)\n",
    "y_pred_svm = svm.predict(X_test.T)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(accuracy_svm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/938], Loss: 0.4337\n",
      "Epoch [1/10], Step [200/938], Loss: 0.2637\n",
      "Epoch [1/10], Step [300/938], Loss: 0.1552\n",
      "Epoch [1/10], Step [400/938], Loss: 0.2181\n",
      "Epoch [1/10], Step [500/938], Loss: 0.2380\n",
      "Epoch [1/10], Step [600/938], Loss: 0.1099\n",
      "Epoch [1/10], Step [700/938], Loss: 0.0993\n",
      "Epoch [1/10], Step [800/938], Loss: 0.1525\n",
      "Epoch [1/10], Step [900/938], Loss: 0.1009\n",
      "Epoch [2/10], Step [100/938], Loss: 0.0430\n",
      "Epoch [2/10], Step [200/938], Loss: 0.2676\n",
      "Epoch [2/10], Step [300/938], Loss: 0.2343\n",
      "Epoch [2/10], Step [400/938], Loss: 0.4767\n",
      "Epoch [2/10], Step [500/938], Loss: 0.1476\n",
      "Epoch [2/10], Step [600/938], Loss: 0.0777\n",
      "Epoch [2/10], Step [700/938], Loss: 0.0345\n",
      "Epoch [2/10], Step [800/938], Loss: 0.0841\n",
      "Epoch [2/10], Step [900/938], Loss: 0.0867\n",
      "Epoch [3/10], Step [100/938], Loss: 0.0905\n",
      "Epoch [3/10], Step [200/938], Loss: 0.1358\n",
      "Epoch [3/10], Step [300/938], Loss: 0.0739\n",
      "Epoch [3/10], Step [400/938], Loss: 0.0190\n",
      "Epoch [3/10], Step [500/938], Loss: 0.0059\n",
      "Epoch [3/10], Step [600/938], Loss: 0.0225\n",
      "Epoch [3/10], Step [700/938], Loss: 0.0327\n",
      "Epoch [3/10], Step [800/938], Loss: 0.0527\n",
      "Epoch [3/10], Step [900/938], Loss: 0.0885\n",
      "Epoch [4/10], Step [100/938], Loss: 0.0852\n",
      "Epoch [4/10], Step [200/938], Loss: 0.0707\n",
      "Epoch [4/10], Step [300/938], Loss: 0.0789\n",
      "Epoch [4/10], Step [400/938], Loss: 0.0339\n",
      "Epoch [4/10], Step [500/938], Loss: 0.0284\n",
      "Epoch [4/10], Step [600/938], Loss: 0.0078\n",
      "Epoch [4/10], Step [700/938], Loss: 0.0210\n",
      "Epoch [4/10], Step [800/938], Loss: 0.1083\n",
      "Epoch [4/10], Step [900/938], Loss: 0.1679\n",
      "Epoch [5/10], Step [100/938], Loss: 0.1111\n",
      "Epoch [5/10], Step [200/938], Loss: 0.0776\n",
      "Epoch [5/10], Step [300/938], Loss: 0.0133\n",
      "Epoch [5/10], Step [400/938], Loss: 0.0176\n",
      "Epoch [5/10], Step [500/938], Loss: 0.0015\n",
      "Epoch [5/10], Step [600/938], Loss: 0.0269\n",
      "Epoch [5/10], Step [700/938], Loss: 0.0595\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0194\n",
      "Epoch [5/10], Step [900/938], Loss: 0.0192\n",
      "Epoch [6/10], Step [100/938], Loss: 0.0087\n",
      "Epoch [6/10], Step [200/938], Loss: 0.0053\n",
      "Epoch [6/10], Step [300/938], Loss: 0.0170\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0233\n",
      "Epoch [6/10], Step [500/938], Loss: 0.0662\n",
      "Epoch [6/10], Step [600/938], Loss: 0.0563\n",
      "Epoch [6/10], Step [700/938], Loss: 0.0077\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0873\n",
      "Epoch [6/10], Step [900/938], Loss: 0.0802\n",
      "Epoch [7/10], Step [100/938], Loss: 0.0025\n",
      "Epoch [7/10], Step [200/938], Loss: 0.0499\n",
      "Epoch [7/10], Step [300/938], Loss: 0.0538\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0035\n",
      "Epoch [7/10], Step [500/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [600/938], Loss: 0.0099\n",
      "Epoch [7/10], Step [700/938], Loss: 0.0635\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0060\n",
      "Epoch [7/10], Step [900/938], Loss: 0.0236\n",
      "Epoch [8/10], Step [100/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [200/938], Loss: 0.0085\n",
      "Epoch [8/10], Step [300/938], Loss: 0.0296\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0031\n",
      "Epoch [8/10], Step [500/938], Loss: 0.0087\n",
      "Epoch [8/10], Step [600/938], Loss: 0.0824\n",
      "Epoch [8/10], Step [700/938], Loss: 0.0162\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0066\n",
      "Epoch [8/10], Step [900/938], Loss: 0.0043\n",
      "Epoch [9/10], Step [100/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [200/938], Loss: 0.0250\n",
      "Epoch [9/10], Step [300/938], Loss: 0.0017\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0228\n",
      "Epoch [9/10], Step [500/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [600/938], Loss: 0.0106\n",
      "Epoch [9/10], Step [700/938], Loss: 0.0129\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0255\n",
      "Epoch [9/10], Step [900/938], Loss: 0.0025\n",
      "Epoch [10/10], Step [100/938], Loss: 0.0025\n",
      "Epoch [10/10], Step [200/938], Loss: 0.0133\n",
      "Epoch [10/10], Step [300/938], Loss: 0.0135\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0228\n",
      "Epoch [10/10], Step [500/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0356\n",
      "Epoch [10/10], Step [700/938], Loss: 0.0061\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0404\n",
      "Epoch [10/10], Step [900/938], Loss: 0.0056\n",
      "Accuracy of the model on the test images: 98.92%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = torchvision.datasets.MNIST('~/.torch/datasets', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST('~/.torch/datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000)\n",
    "\n",
    "class MnistLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(MnistLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).cuda()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).cuda()\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = MnistLSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 28, 28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28, 28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {:.2f}%'.format(100 * correct / total))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}